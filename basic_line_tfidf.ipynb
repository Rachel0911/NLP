{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains 1-gram tfidf without any other text preprocessing. And 2-grams tfidf just with removing low frequence 2-grams. Both use logistic regresson method.\n",
    "* logistic regresson over bage of 1-gram with tfidf has 88.3% accuracy for prediction\n",
    "* logistic regresson over bage of 2-grams with tfidf has 89.2%(increase 0.9%) accuracy for prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let us look at details about logistic regression over bag of 1-gram with tfidf.\n",
    "\n",
    "**What is the tfidf?**    \n",
    "\n",
    "From Wikipedia: TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. \n",
    "\n",
    "Example:  \n",
    "Consider we have a document containing 100 words where the word \"business\" appears 10 times. For \"business\", the term frequency is (10 / 100) = 0.1. Addition, assume we have 10 thousand documents and the word \"business\" appears in one hundred of these. The inverse document frequency is log(10000 / 100) = 2. Thus, the Tf-idf weight of \"business\" is their product 0.1 * 2 = 0.2.\n",
    "\n",
    "In practice, we don't calculate tfidf weights for every word in our dataset. In sklearn, we could simply use TfidfVectorizer to do it.\n",
    "\n",
    "**Why do I choose logistic regression?**    \n",
    "\n",
    "After we take bag of 1-grams with TF-IDF, we will get a matrix of features which is 25,000 rows and 75,000 columns.  It is an extremely sparse matrix since 99.8% of all values are zero.\n",
    "A linear model can handle sparse data and this is kind of binary classification problem. Thus, I choose logistic regression model.  \n",
    "\n",
    "Let's look at the code for logistic regression over bag of 1-gram with tfidf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import os\n",
    "# import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all train reviews to reviews_train list\n",
    "reviews_train = []\n",
    "for line in open('movie_data/full_train.txt'): \n",
    "    reviews_train.append(line.strip())\n",
    "\n",
    "# store all test reviews to reviews_train list\n",
    "reviews_test = []\n",
    "for line in open('movie_data/full_test.txt'):  \n",
    "    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take bag of 1-gram with tfidf\n",
    "tfidf = TfidfVectorizer()\n",
    "features = tfidf.fit_transform(reviews_train)\n",
    "X = pd.DataFrame(features.todense(),\n",
    "            columns=tfidf.get_feature_names())\n",
    "X_test = tfidf.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7936\n",
      "Accuracy for C=0.05: 0.83152\n",
      "Accuracy for C=0.25: 0.86832\n",
      "Accuracy for C=0.5: 0.87968\n",
      "Accuracy for C=1: 0.88672\n"
     ]
    }
   ],
   "source": [
    "# first 12500 rows are positive reviews, others are negative reviews\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "# do cross_validation to find the best C in logistic regression\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
    "\n",
    "# try multiple C in logisticregression and find the best which will be used to predict\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88316\n"
     ]
    }
   ],
   "source": [
    "# C=1 has the highest accuracy of validation, use final_model to predict\n",
    "# the accuracy for test is 88.3% not bad, it is better than random classification(50%)\n",
    "final_model = LogisticRegression(C=1)\n",
    "final_model.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(target, final_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please look at top positive words. They are great, excellent, best, perfect, wonderful. It means the model captured positive sentiment under the case where it knows nothing about English. The model also learns negative sentiment, if we look at top negative words: worst, bad, awful, waste, boring. **Pretty Cool!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top positive word:  ('great', 7.597249852954926)\n",
      "top positive word:  ('excellent', 6.184830619925593)\n",
      "top positive word:  ('best', 5.127475318155725)\n",
      "top positive word:  ('perfect', 4.818259658999337)\n",
      "top positive word:  ('wonderful', 4.675988654271976)\n",
      "------------------------------------------------------------\n",
      "top negative word:  ('worst', -9.241902496068317)\n",
      "top negative word:  ('bad', -7.9557462534905765)\n",
      "top negative word:  ('awful', -6.4928601936902535)\n",
      "top negative word:  ('waste', -6.278174894676147)\n",
      "top negative word:  ('boring', -6.019791640101872)\n"
     ]
    }
   ],
   "source": [
    "# find the top positive words and negative words\n",
    "feature_to_coef = {word: coef for word, coef in zip(tfidf.get_feature_names(), final_model.coef_[0])}\n",
    "\n",
    "positives = sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)\n",
    "for best_positive in positives[:5]:\n",
    "    print (\"top positive word: \", best_positive)\n",
    "\n",
    "print('-'*60)\n",
    "negatives = sorted(feature_to_coef.items(), key=lambda x: x[1])\n",
    "    \n",
    "for best_negative in negatives[:5]:\n",
    "    print (\"top negative word: \", best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try to improve model. Let us look at details about logistic regression over bag of 2-grams with tfidf.  \n",
    "This time we throw away some 2-grams that appear less than 5 times in all documents. Because these 2-grams are likely either typos or people don't say like that, and some of them don't make any sense. After throw away low frequence 2-grams, we will get a matrix with 25000 rows and 156821 columns.  \n",
    "This time just use C=1 for logistic regression since if we use cross_validation to choose the best C like in 1-gram, the kernel will die and restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-grams throw away n-grams less than 5 times\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=5)\n",
    "features = tfidf.fit_transform(reviews_train)\n",
    "X = pd.DataFrame(features.todense(),\n",
    "            columns=tfidf.get_feature_names())\n",
    "X_test = tfidf.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.89228\n"
     ]
    }
   ],
   "source": [
    "# the accuracy for test is 89.2% increased\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "model = LogisticRegression(C=1)\n",
    "model.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" % accuracy_score(target, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top positive word:  ('great', 7.663454840228788)\n",
      "top positive word:  ('excellent', 5.315721883208201)\n",
      "top positive word:  ('wonderful', 4.363274665123357)\n",
      "top positive word:  ('best', 4.132473357265538)\n",
      "top positive word:  ('perfect', 4.124617663286926)\n",
      "------------------------------------------------------------\n",
      "top negative word:  ('bad', -8.448189376176252)\n",
      "top negative word:  ('worst', -7.323039515276972)\n",
      "top negative word:  ('the worst', -5.867879346841515)\n",
      "top negative word:  ('awful', -5.76580591273964)\n",
      "top negative word:  ('boring', -5.360840023004068)\n"
     ]
    }
   ],
   "source": [
    "# find the top positive n-grams and negative n-grams\n",
    "# We saw one 2-grams(the worst) in top negative ones.\n",
    "feature_to_coef = {word: coef for word, coef in zip(tfidf.get_feature_names(), model.coef_[0])}\n",
    "\n",
    "positives = sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)\n",
    "for best_positive in positives[:5]:\n",
    "    print (\"top positive word: \", best_positive)\n",
    "\n",
    "print('-'*60)\n",
    "negatives = sorted(feature_to_coef.items(), key=lambda x: x[1])\n",
    "    \n",
    "for best_negative in negatives[:5]:\n",
    "    print (\"top negative word: \", best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
